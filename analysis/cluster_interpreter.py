"""
M√≥dulo avan√ßado de interpreta√ß√£o de clusters para SOM (Self-Organizing Maps)
Vers√£o melhorada com an√°lises mais robustas e visualiza√ß√µes detalhadas
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from typing import Dict, Tuple, Optional, Any
from scipy import stats
from analysis.cluster_evaluator import ClusterQualityEvaluator

# Configura√ß√£o de estilo para visualiza√ß√µes
plt.style.use('default')
sns.set_palette("husl")

logger = logging.getLogger(__name__)

class AdvancedSOMClusterInterpreter:
    """Interpretador avan√ßado de clusters baseado em SOM com an√°lises detalhadas"""
    
    def __init__(self, preprocessor, som_trainer, som_analyzer):
        self.preprocessor = preprocessor
        self.som_trainer = som_trainer
        self.som_analyzer = som_analyzer
        self.cluster_profiles = {}
        self.quality_evaluator = ClusterQualityEvaluator()
        self.feature_names = None

    def analyze_som_clusters(self, X, original_df, max_clusters=15, 
                           min_cluster_size_ratio=0.001, 
                           noise_threshold=0.10) -> Tuple[pd.DataFrame, Dict]:
        """
        Analisa clusters baseados no SOM com balanceamento avan√ßado
        
        Args:
            X: DataFrame com features
            original_df: DataFrame original com dados completos
            max_clusters: N√∫mero m√°ximo de clusters
            min_cluster_size_ratio: Raz√£o m√≠nima do tamanho do cluster
            noise_threshold: Threshold para considerar como ru√≠do
            
        Returns:
            Tuple com DataFrame enriquecido e m√©tricas de qualidade
        """
        logger.info("üîç AN√ÅLISE AVAN√áADA DE CLUSTERS DO SOM")
        
        if self.som_trainer.som is None:
            raise ValueError("Rede de Kohonen n√£o treinada!")

        # Prepara√ß√£o dos dados
        data = X.values.astype(np.float32)
        self.feature_names = X.columns.tolist()

        # Obter clusters naturais do SOM
        neuron_clusters = self.som_analyzer.get_neuron_clusters()
        if neuron_clusters is None:
            raise ValueError("Clusters naturais n√£o foram calculados!")

        # Atribui√ß√£o balanceada de clusters
        balanced_clusters, cluster_metrics = self._advanced_cluster_assignment(
            data, neuron_clusters, max_clusters, 
            min_cluster_size_ratio, noise_threshold
        )

        # Preparar DataFrame final
        result_df = self._prepare_result_dataframe(original_df, balanced_clusters)
        
        # An√°lises completas
        quality_metrics = self._comprehensive_analysis(result_df, data, balanced_clusters)
        
        return result_df, {**quality_metrics, **cluster_metrics}

    def _advanced_cluster_assignment(self, data, neuron_clusters, max_clusters, 
                                   min_cluster_size_ratio, noise_threshold):
        """Atribui√ß√£o avan√ßada de clusters com m√∫ltiplas estrat√©gias"""
        logger.info("   ‚öñÔ∏è  Atribui√ß√£o avan√ßada de clusters...")
        
        som = self.som_trainer.som
        neuron_cluster_map = self._create_neuron_cluster_map(neuron_clusters)
        
        # Atribui√ß√£o inicial
        initial_assignments = self._get_initial_assignments(som, data, neuron_cluster_map)
        
        # An√°lise da distribui√ß√£o inicial
        cluster_stats = self._analyze_initial_distribution(initial_assignments)
        
        # Estrat√©gias de balanceamento
        balanced_assignments = self._apply_balancing_strategies(
            initial_assignments, cluster_stats, len(data), 
            max_clusters, min_cluster_size_ratio, noise_threshold
        )
        
        # M√©tricas do processo
        cluster_metrics = {
            'initial_clusters': len(cluster_stats['valid_clusters']),
            'final_clusters': len(np.unique(balanced_assignments)) - 1,  # Excluir ru√≠do
            'noise_points': np.sum(np.array(balanced_assignments) == 0),
            'retained_data_ratio': np.sum(np.array(balanced_assignments) > 0) / len(data)
        }
        
        logger.info(f"   ‚úÖ Balanceamento conclu√≠do: {cluster_metrics['final_clusters']} clusters")
        
        return balanced_assignments, cluster_metrics

    def _create_neuron_cluster_map(self, neuron_clusters):
        """Cria mapeamento neur√¥nio -> cluster"""
        neuron_cluster_map = {}
        for i in range(neuron_clusters.shape[0]):
            for j in range(neuron_clusters.shape[1]):
                cluster_id = neuron_clusters[i, j]
                if cluster_id > 0:
                    neuron_cluster_map[(i, j)] = cluster_id
        return neuron_cluster_map

    def _get_initial_assignments(self, som, data, neuron_cluster_map):
        """Obt√©m atribui√ß√µes iniciais dos clusters"""
        assignments = []
        for sample in data:
            winner = som.winner(sample)
            cluster_id = neuron_cluster_map.get(winner, 0)
            assignments.append(cluster_id)
        return assignments

    def _analyze_initial_distribution(self, assignments):
        """Analisa distribui√ß√£o inicial dos clusters"""
        assignments_array = np.array(assignments)
        unique_clusters, counts = np.unique(assignments_array, return_counts=True)
        
        valid_clusters = []
        cluster_sizes = {}
        
        for cluster_id, count in zip(unique_clusters, counts):
            cluster_sizes[cluster_id] = count
            if cluster_id > 0:  # Excluir cluster 0 (ru√≠do)
                valid_clusters.append(cluster_id)
        
        return {
            'unique_clusters': unique_clusters,
            'counts': counts,
            'valid_clusters': valid_clusters,
            'cluster_sizes': cluster_sizes
        }

    def _apply_balancing_strategies(self, assignments, cluster_stats, total_points,
                                    max_clusters, min_cluster_size_ratio, noise_threshold):
        """
        ‚úÖ BALANCEAMENTO ULTRA-FLEX√çVEL
        """
        # ‚úÖ MUDAN√áA: Threshold MUITO mais baixo
        min_cluster_size = max(100, int(total_points * 0.0005))  # 0.05% ou 100 pontos

        assignments_array = np.array(assignments)

        logger.info(f"\n   üìä AN√ÅLISE DE BALANCEAMENTO:")
        logger.info(f"      ‚Ä¢ Total de pontos: {total_points:,}")
        logger.info(f"      ‚Ä¢ Tamanho m√≠nimo: {min_cluster_size:,}")

        # Identificar clusters v√°lidos
        valid_clusters = []
        small_clusters = []
        cluster_info = []

        for cluster_id in cluster_stats['valid_clusters']:
            cluster_size = cluster_stats['cluster_sizes'][cluster_id]
            percentage = (cluster_size / total_points) * 100

            cluster_info.append({
                'id': cluster_id,
                'size': cluster_size,
                'percentage': percentage
            })

            if cluster_size >= min_cluster_size:
                valid_clusters.append(cluster_id)
                logger.info(f"      ‚úÖ Cluster {cluster_id}: {cluster_size:,} ({percentage:.2f}%) - V√ÅLIDO")
            else:
                small_clusters.append(cluster_id)
                logger.info(f"      ‚ö†Ô∏è  Cluster {cluster_id}: {cluster_size:,} ({percentage:.2f}%) - PEQUENO")

        # ‚úÖ NOVO: Se nenhum cluster v√°lido, for√ßar os 5 maiores
        if len(valid_clusters) == 0:
            logger.warning("      ‚ö†Ô∏è  NENHUM cluster v√°lido! For√ßando os maiores...")
            sorted_clusters = sorted(cluster_info, key=lambda x: x['size'], reverse=True)
            num_to_keep = min(5, len(sorted_clusters))
            valid_clusters = [c['id'] for c in sorted_clusters[:num_to_keep]]

            for c in sorted_clusters[:num_to_keep]:
                logger.info(f"      üîÑ FOR√áADO Cluster {c['id']}: {c['size']:,} ({c['percentage']:.2f}%)")

        # Limitar n√∫mero m√°ximo
        if len(valid_clusters) > max_clusters:
            logger.info(f"      ‚úÇÔ∏è  Limitando de {len(valid_clusters)} para {max_clusters} clusters")
            sorted_valid = sorted(cluster_info, key=lambda x: x['size'], reverse=True)
            valid_clusters = [c['id'] for c in sorted_valid[:max_clusters] if c['id'] in valid_clusters]

        # Reatribuir pontos
        balanced_assignments = []
        reallocated = 0
        noise_count = 0

        for assignment in assignments:
            if assignment == 0:
                balanced_assignments.append(0)
                noise_count += 1
            elif assignment not in valid_clusters:
                # Realocar para o cluster v√°lido mais pr√≥ximo (por tamanho)
                if len(valid_clusters) > 0:
                    # Simplifica√ß√£o: usar o maior cluster
                    largest = max(valid_clusters, key=lambda x: cluster_stats['cluster_sizes'][x])
                    balanced_assignments.append(largest)
                    reallocated += 1
                else:
                    balanced_assignments.append(0)
                    noise_count += 1
            else:
                balanced_assignments.append(assignment)

        logger.info(f"\n   ‚úÖ BALANCEAMENTO CONCLU√çDO:")
        logger.info(f"      ‚Ä¢ Clusters v√°lidos: {len(valid_clusters)}")
        logger.info(f"      ‚Ä¢ IDs: {sorted(valid_clusters)}")
        logger.info(f"      ‚Ä¢ Realocados: {reallocated:,}")
        logger.info(f"      ‚Ä¢ Ru√≠do: {noise_count:,} ({noise_count / total_points * 100:.1f}%)")

        return balanced_assignments

    def _prepare_result_dataframe(self, original_df, clusters):
        """Prepara DataFrame final com clusters e an√°lises"""
        result_df = original_df.iloc[:len(clusters)].copy()
        result_df['CLUSTER_SOM'] = clusters
        result_df['CLUSTER_SIZE'] = result_df['CLUSTER_SOM'].map(
            result_df['CLUSTER_SOM'].value_counts()
        )
        return result_df

    def _comprehensive_analysis(self, df, data, clusters):
        """Executa an√°lise completa dos clusters"""
        logger.info("   üìä Iniciando an√°lise compreensiva...")
        
        # An√°lise de qualidade
        quality_metrics = self.quality_evaluator.comprehensive_cluster_quality(
            data, clusters, self.som_trainer.som
        )
        
        # An√°lises detalhadas
        self._advanced_cluster_distribution_analysis(df)
        self._cluster_characteristics_analysis(df)
        self._create_comprehensive_visualizations(df, data, clusters)
        
        return quality_metrics

    def _advanced_cluster_distribution_analysis(self, df):
        """An√°lise avan√ßada da distribui√ß√£o de clusters"""
        logger.info("\nüìä DISTRIBUI√á√ÉO AVAN√áADA DOS CLUSTERS")
        
        cluster_dist = df['CLUSTER_SOM'].value_counts().sort_index()
        valid_clusters = cluster_dist[cluster_dist.index != 0]
        
        if len(valid_clusters) == 0:
            logger.warning("   ‚ö†Ô∏è  Nenhum cluster v√°lido encontrado!")
            return
        
        # Estat√≠sticas detalhadas
        total_records = len(df)
        noise_count = cluster_dist.get(0, 0)
        clustered_records = valid_clusters.sum()
        
        logger.info(f"   ‚Ä¢ Clusters v√°lidos: {len(valid_clusters)}")
        logger.info(f"   ‚Ä¢ Registros em clusters: {clustered_records:,} ({clustered_records/total_records*100:.1f}%)")
        logger.info(f"   ‚Ä¢ Registros como ru√≠do: {noise_count:,} ({noise_count/total_records*100:.1f}%)")
        logger.info(f"   ‚Ä¢ Tamanho m√©dio do cluster: {valid_clusters.mean():.0f} registros")
        logger.info(f"   ‚Ä¢ Desvio padr√£o: {valid_clusters.std():.0f} registros")
        
        # Identificar clusters outliers
        Q1 = valid_clusters.quantile(0.25)
        Q3 = valid_clusters.quantile(0.75)
        IQR = Q3 - Q1
        outlier_threshold = Q3 + 1.5 * IQR
        outliers = valid_clusters[valid_clusters > outlier_threshold]
        
        if len(outliers) > 0:
            logger.info(f"   ‚Ä¢ Clusters grandes (outliers): {list(outliers.index)}")

    def _cluster_characteristics_analysis(self, df):
        """An√°lise detalhada das caracter√≠sticas dos clusters"""
        logger.info("\nüìà AN√ÅLISE DETALHADA POR CLUSTER")
        
        valid_clusters = sorted([c for c in df['CLUSTER_SOM'].unique() if c != 0])
        
        if not valid_clusters:
            logger.warning("   ‚ö†Ô∏è  Nenhum cluster v√°lido para an√°lise!")
            return
        
        for cluster_id in valid_clusters:
            cluster_data = df[df['CLUSTER_SOM'] == cluster_id]
            self._analyze_single_cluster(cluster_data, cluster_id, df)

    def _analyze_single_cluster(self, cluster_data, cluster_id, full_df):
        """Analisa um cluster individual"""
        size = len(cluster_data)
        percentage = (size / len(full_df)) * 100
        
        logger.info(f"\nüéØ CLUSTER {cluster_id}: {size:,} registros ({percentage:.1f}%)")
        logger.info("   " + "‚îÄ" * 50)
        
        # An√°lise de features categ√≥ricas
        self._analyze_categorical_features(cluster_data, size)
        
        # An√°lise de features num√©ricas
        self._analyze_numeric_features(cluster_data, full_df)

    def _analyze_categorical_features(self, cluster_data, cluster_size):
        """Analisa features categ√≥ricas do cluster"""
        categorical_insights = {}
        
        for col in cluster_data.select_dtypes(include=['object', 'category']).columns:
            if cluster_data[col].nunique() < 15:  # Limite para evitar alta cardinalidade
                value_counts = cluster_data[col].value_counts()
                top_value = value_counts.head(2)  # Top 2 valores
                
                for value, count in top_value.items():
                    percentage = (count / cluster_size) * 100
                    if percentage > 20:  # Threshold mais baixo para capturar padr√µes
                        if col not in categorical_insights:
                            categorical_insights[col] = []
                        categorical_insights[col].append((value, percentage))
        
        if categorical_insights:
            logger.info("   üè∑Ô∏è  CARACTER√çSTICAS CATEG√ìRICAS:")
            for col, values in list(categorical_insights.items())[:8]:
                insights_str = ", ".join([f"{val} ({pct:.1f}%)" for val, pct in values[:2]])
                logger.info(f"     ‚Ä¢ {col}: {insights_str}")

    def _analyze_numeric_features(self, cluster_data, full_df):
        """Analisa features num√©ricas com compara√ß√£o global"""
        numeric_cols = cluster_data.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) == 0:
            return

        logger.info("   üìä CARACTER√çSTICAS NUM√âRICAS:")

        for col in list(numeric_cols)[:6]:  # Limitar para n√£o poluir
            cluster_mean = cluster_data[col].mean()
            global_mean = full_df[col].mean()

            # ‚úÖ VALIDA√á√ÉO: Se coordenadas ainda corrompidas, avisar
            if col in ['LATITUDE', 'LONGITUDE'] and abs(cluster_mean) > 1000:
                logger.warning(f"      ‚ö†Ô∏è  {col}: AINDA CORROMPIDO ({cluster_mean:.0f})")
                logger.warning(f"          Aplicar corre√ß√£o de escala no preprocessor!")
                continue

            difference_pct = ((cluster_mean - global_mean) / abs(global_mean)) * 100 if global_mean != 0 else 0

            significance = "‚Üë‚Üë" if difference_pct > 15 else "‚Üì‚Üì" if difference_pct < -15 else "‚âà"

            logger.info(f"     ‚Ä¢ {col}: {significance} avg={cluster_mean:.1f} "
                        f"(global: {global_mean:.1f}, diff: {difference_pct:+.1f}%)")

    def _create_comprehensive_visualizations(self, df, data, clusters):
        """Cria visualiza√ß√µes abrangentes dos clusters"""
        logger.info("   üé® Criando visualiza√ß√µes...")
        
        fig = plt.figure(figsize=(20, 16))
        
        # 1. Distribui√ß√£o de clusters
        ax1 = plt.subplot(2, 3, 1)
        self._plot_cluster_distribution(df, ax1)
        
        # 2. Composi√ß√£o dos clusters (features principais)
        ax2 = plt.subplot(2, 3, 2)
        self._plot_cluster_composition(df, ax2)
        
        # 3. Heatmap de caracter√≠sticas
        ax3 = plt.subplot(2, 3, 3)
        self._plot_feature_heatmap(df, ax3)
        
        # 4. Dimensionalidade reduzida (se dispon√≠vel)
        ax4 = plt.subplot(2, 3, 4)
        self._plot_projection(df, data, clusters, ax4)
        
        # 5. Tamanho dos clusters vs qualidade
        ax5 = plt.subplot(2, 3, 5)
        self._plot_cluster_quality(df, ax5)
        
        # 6. Matriz de correla√ß√£o entre clusters
        ax6 = plt.subplot(2, 3, 6)
        self._plot_cluster_correlation(df, ax6)
        
        plt.tight_layout()
        plt.savefig('advanced_som_cluster_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Gr√°fico adicional: Radar chart para perfis de cluster
        self._create_radar_chart(df)

    def _plot_cluster_distribution(self, df, ax):
        """Plot da distribui√ß√£o de clusters"""
        cluster_dist = df['CLUSTER_SOM'].value_counts().sort_index()
        valid_clusters = cluster_dist[cluster_dist.index != 0]
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(valid_clusters)))
        bars = ax.bar(range(len(valid_clusters)), valid_clusters.values, color=colors)
        
        ax.set_title('Distribui√ß√£o de Clusters', fontsize=14, fontweight='bold')
        ax.set_xlabel('Cluster ID')
        ax.set_ylabel('N√∫mero de Registros')
        ax.set_xticks(range(len(valid_clusters)))
        ax.set_xticklabels(valid_clusters.index, rotation=45)
        
        # Adicionar valores
        for bar, count in zip(bars, valid_clusters.values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{count:,}', ha='center', va='bottom', fontsize=9)

    def _plot_cluster_composition(self, df, ax):
        """Plot da composi√ß√£o dos clusters por features principais"""
        # Implementar an√°lise de features mais importantes por cluster
        valid_clusters = sorted([c for c in df['CLUSTER_SOM'].unique() if c != 0])
        
        if len(valid_clusters) == 0:
            ax.text(0.5, 0.5, 'Sem clusters v√°lidos', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Composi√ß√£o dos Clusters', fontsize=14)
            return
        
        # Exemplo simplificado - adaptar conforme necessidade
        composition_data = []
        for cluster_id in valid_clusters[:5]:  # Limitar a 5 clusters
            cluster_data = df[df['CLUSTER_SOM'] == cluster_id]
            # Calcular m√©tricas de composi√ß√£o aqui
            
        ax.set_title('Composi√ß√£o dos Clusters\n(Top Features)', fontsize=14, fontweight='bold')

    def _plot_feature_heatmap(self, df, ax):
        """Heatmap de caracter√≠sticas dos clusters"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            # Calcular m√©dias por cluster para heatmap
            cluster_means = df.groupby('CLUSTER_SOM')[numeric_cols[:8]].mean()  # Top 8 features
            
            if len(cluster_means) > 1:
                # Normalizar para melhor visualiza√ß√£o
                normalized_means = (cluster_means - cluster_means.mean()) / cluster_means.std()
                sns.heatmap(normalized_means.iloc[1:], ax=ax, cmap='RdBu_r', center=0, 
                           annot=True, fmt='.2f', cbar_kws={'label': 'Z-score'})
                ax.set_title('Heatmap de Caracter√≠sticas\n(Normalizado)', fontsize=14, fontweight='bold')
            else:
                ax.text(0.5, 0.5, 'Dados insuficientes\npara heatmap', 
                       ha='center', va='center', transform=ax.transAxes)
        else:
            ax.text(0.5, 0.5, 'Sem features num√©ricas', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Heatmap de Caracter√≠sticas', fontsize=14)

    def _plot_projection(self, df, data, clusters, ax):
        """Proje√ß√£o dos clusters em 2D (se dispon√≠vel)"""
        try:
            from sklearn.manifold import TSNE
            from sklearn.decomposition import PCA
            
            # Usar PCA ou t-SNE para proje√ß√£o
            if data.shape[1] > 2:
                projector = PCA(n_components=2, random_state=42)
                projection = projector.fit_transform(data)
                title = 'Proje√ß√£o PCA dos Clusters'
            else:
                projection = data
                title = 'Visualiza√ß√£o Direta dos Clusters'
            
            scatter = ax.scatter(projection[:, 0], projection[:, 1], 
                               c=clusters, cmap='tab10', alpha=0.6, s=30)
            ax.set_title(title, fontsize=14, fontweight='bold')
            ax.set_xlabel('Componente 1')
            ax.set_ylabel('Componente 2')
            
            # Adicionar legenda para clusters
            plt.colorbar(scatter, ax=ax, label='Cluster ID')
            
        except ImportError:
            ax.text(0.5, 0.5, 'Scikit-learn n√£o dispon√≠vel\npara proje√ß√£o', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Proje√ß√£o dos Clusters', fontsize=14)

    def _plot_cluster_quality(self, df, ax):
        """Plot de qualidade vs tamanho dos clusters"""
        valid_clusters = [c for c in df['CLUSTER_SOM'].unique() if c != 0]
        
        if len(valid_clusters) < 2:
            ax.text(0.5, 0.5, 'Clusters insuficientes\npara an√°lise', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Qualidade vs Tamanho', fontsize=14)
            return
        
        cluster_sizes = []
        cluster_qualities = []  # M√©tricas de qualidade podem ser adicionadas
        
        for cluster_id in valid_clusters:
            cluster_data = df[df['CLUSTER_SOM'] == cluster_id]
            cluster_sizes.append(len(cluster_data))
            # Calcular m√©tricas de qualidade aqui
        
        ax.scatter(cluster_sizes, cluster_qualities if cluster_qualities else cluster_sizes, 
                  alpha=0.6, s=60)
        ax.set_xlabel('Tamanho do Cluster')
        ax.set_ylabel('M√©trica de Qualidade' if cluster_qualities else 'Tamanho')
        ax.set_title('Rela√ß√£o: Tamanho vs Qualidade', fontsize=14, fontweight='bold')

    def _plot_cluster_correlation(self, df, ax):
        """Matriz de correla√ß√£o entre clusters"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            ax.text(0.5, 0.5, 'Features insuficientes\npara correla√ß√£o', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Correla√ß√£o entre Clusters', fontsize=14)
            return
        
        # Calcular correla√ß√µes m√©dias entre clusters
        valid_clusters = [c for c in df['CLUSTER_SOM'].unique() if c != 0]
        
        if len(valid_clusters) < 2:
            ax.text(0.5, 0.5, 'Clusters insuficientes', 
                   ha='center', va='center', transform=ax.transAxes)
            return
        
         # Calcula as m√©dias das features num√©ricas por cluster
        cluster_means = []
        cluster_labels = []
    
        for c in valid_clusters:
            cluster_data = df[df['CLUSTER_SOM'] == c]
            if len(cluster_data) > 1:  # Precisa ter pelo menos 2 pontos
                # Calcula a m√©dia das features num√©ricas para este cluster
                means = cluster_data[numeric_cols].mean().values
                cluster_means.append(means)
                cluster_labels.append(c)
    
            if len(cluster_means) < 2:
                ax.text(0.5, 0.5, 'Dados insuficientes\npara correla√ß√£o', 
                    ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Correla√ß√£o entre Clusters', fontsize=14)
            return
    
    # Agora todas as arrays t√™m o mesmo comprimento (n√∫mero de features)
        cluster_means_array = np.array(cluster_means)  # Shape: (n_clusters, n_features)
    
    # Calcula correla√ß√£o entre os perfis m√©dios dos clusters
        correlation_matrix = np.corrcoef(cluster_means_array)
    
        im = ax.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)
        ax.set_xticks(range(len(cluster_labels)))
        ax.set_yticks(range(len(cluster_labels)))
        ax.set_xticklabels(cluster_labels)
        ax.set_yticklabels(cluster_labels)
        ax.set_title('Similaridade entre Clusters\n(Correla√ß√£o dos Perfis M√©dias)', 
                fontsize=14, fontweight='bold')
    
        # Adiciona valores na matriz
        for i in range(len(cluster_labels)):
            for j in range(len(cluster_labels)):
                ax.text(j, i, f'{correlation_matrix[i, j]:.2f}',
                   ha='center', va='center', 
                   color='white' if abs(correlation_matrix[i, j]) > 0.5 else 'black',
                   fontsize=9)
    
        plt.colorbar(im, ax=ax, label='Coeficiente de Correla√ß√£o')

    def _create_radar_chart(self, df):
        """Cria gr√°fico radar para perfis de clusters"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:6]  # Top 6 features
            
            if len(numeric_cols) < 3:
                return
                
            valid_clusters = [c for c in df['CLUSTER_SOM'].unique() if c != 0][:8]  # Top 8 clusters
            
            if len(valid_clusters) < 2:
                return
            
            # Preparar dados para radar chart
            fig = plt.figure(figsize=(12, 8))
            ax = fig.add_subplot(111, polar=True)
            
            # √Çngulos para cada feature
            angles = np.linspace(0, 2*np.pi, len(numeric_cols), endpoint=False).tolist()
            angles += angles[:1]  # Fechar o c√≠rculo
            
            for cluster_id in valid_clusters:
                cluster_data = df[df['CLUSTER_SOM'] == cluster_id]
                values = cluster_data[numeric_cols].mean().tolist()
                values += values[:1]  # Fechar o c√≠rculo
                
                ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cluster_id}')
                ax.fill(angles, values, alpha=0.1)
            
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(numeric_cols)
            ax.set_title('Perfil dos Clusters - Radar Chart', size=14, fontweight='bold')
            ax.legend(bbox_to_anchor=(1.1, 1.1))
            
            plt.savefig('cluster_radar_chart.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  N√£o foi poss√≠vel criar radar chart: {e}")

    def get_cluster_profiles(self) -> Dict:
        """Retorna perfis detalhados dos clusters"""
        return self.cluster_profiles

    def generate_cluster_report(self, df, output_file='cluster_analysis_report.txt'):
        """Gera relat√≥rio completo da an√°lise"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("RELAT√ìRIO DE AN√ÅLISE DE CLUSTERS - SOM\n")
            f.write("=" * 50 + "\n\n")
            
            # Estat√≠sticas b√°sicas
            cluster_dist = df['CLUSTER_SOM'].value_counts().sort_index()
            valid_clusters = cluster_dist[cluster_dist.index != 0]
            
            f.write(f"ESTAT√çSTICAS GERAIS:\n")
            f.write(f"- Total de clusters v√°lidos: {len(valid_clusters)}\n")
            f.write(f"- Total de registros: {len(df):,}\n")
            f.write(f"- Registros em clusters: {valid_clusters.sum():,}\n")
            f.write(f"- Registros como ru√≠do: {cluster_dist.get(0, 0):,}\n\n")
            
            # Perfil de cada cluster
            f.write("PERFIS DOS CLUSTERS:\n")
            f.write("-" * 30 + "\n")
            
            for cluster_id in valid_clusters.index:
                cluster_data = df[df['CLUSTER_SOM'] == cluster_id]
                size = len(cluster_data)
                percentage = (size / len(df)) * 100
                
                f.write(f"\nCLUSTER {cluster_id} ({size:,} registros - {percentage:.1f}%):\n")
                
                # Features mais importantes
                for col in cluster_data.select_dtypes(include=['object', 'category']).columns[:3]:
                    if cluster_data[col].nunique() < 10:
                        top_value = cluster_data[col].value_counts().head(1)
                        if len(top_value) > 0:
                            value, count = top_value.index[0], top_value.values[0]
                            pct = (count / size) * 100
                            f.write(f"  ‚Ä¢ {col}: {value} ({pct:.1f}%)\n")
        
        logger.info(f"   üìÑ Relat√≥rio salvo em: {output_file}")

# Vers√£o de compatibilidade para c√≥digo existente
SOMClusterInterpreter = AdvancedSOMClusterInterpreter