"""
Advanced SOM cluster interpreter (hardened)
- Noise handling with adaptive thresholds
- BMU-based reassignment for small/invalid clusters
- Defensive coding (blindagem) for large datasets and edge cases
- Top features summary per cluster
"""

import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from typing import Dict, Tuple, Optional, Any, List
from scipy import stats

# IMPORTANT: uses your optimized evaluator
from analysis.cluster_evaluator import ClusterQualityEvaluator

# Visualization style
plt.style.use('default')
sns.set_palette("husl")

logger = logging.getLogger(__name__)


class AdvancedSOMClusterInterpreter:
    """Interpretador avan√ßado de clusters baseado em SOM com an√°lises detalhadas e blindagem"""

    def __init__(self, preprocessor, som_trainer, som_analyzer):
        self.preprocessor = preprocessor
        self.som_trainer = som_trainer
        self.som_analyzer = som_analyzer
        self.cluster_profiles = {}
        self.quality_evaluator = ClusterQualityEvaluator()
        self.feature_names = None

    def analyze_som_clusters(
        self,
        X: pd.DataFrame,
        original_df: pd.DataFrame,
        max_clusters: int = 15,
        min_cluster_size_ratio: float = 0.001,
        noise_threshold: float = 0.10,
        reassign_noise: bool = True,
        bmu_batch: int = 10000,
        top_n_features: int = 15
    ) -> Tuple[pd.DataFrame, Dict]:
        """
        Analisa clusters baseados no SOM com:
        - Tratamento de ru√≠dos (ID 0) com limiar adaptativo
        - Reatribui√ß√£o BMU de clusters pequenos/invalidos
        - Blindagem para dados grandes e m√©tricas
        - Top features por cluster

        Args:
            X: DataFrame com features (p√≥s-preprocessamento)
            original_df: DataFrame original com dados completos
            max_clusters: N√∫mero m√°ximo de clusters a manter
            min_cluster_size_ratio: Raz√£o m√≠nima do tamanho do cluster (fallback se n√£o usar absoluto)
            noise_threshold: M√°ximo de ru√≠do aceito (propor√ß√£o)
            reassign_noise: Se True, tenta realocar parte dos ru√≠dos por BMU
            bmu_batch: Tamanho de lote para c√°lculo de BMUs (mem√≥ria segura)
            top_n_features: N√∫mero de top features na an√°lise

        Returns:
            result_df: DataFrame enriquecido com CLUSTER_SOM e CLUSTER_SIZE
            metrics: Dicion√°rio com m√©tricas de qualidade e processo
        """
        logger.info("üîç AN√ÅLISE AVAN√áADA DE CLUSTERS DO SOM (HARDENED)")

        # Blindagem: checagens de pr√©-condi√ß√£o
        if self.som_trainer is None or self.som_trainer.som is None:
            raise ValueError("Rede de Kohonen n√£o treinada!")
        if self.som_analyzer is None:
            raise ValueError("Som analyzer n√£o dispon√≠vel!")
        neuron_clusters = getattr(self.som_analyzer, "get_neuron_clusters", None)
        if neuron_clusters is None:
            raise ValueError("Som analyzer sem m√©todo get_neuron_clusters!")
        neuron_clusters = self.som_analyzer.get_neuron_clusters()
        if neuron_clusters is None:
            raise ValueError("Clusters naturais n√£o foram calculados!")

        # Prepara√ß√£o dos dados
        data = X.values.astype(np.float32)
        self.feature_names = X.columns.tolist()
        som = self.som_trainer.som

        # Mapeamento neur√¥nio -> cluster
        neuron_cluster_map = self._create_neuron_cluster_map(neuron_clusters)

        # Atribui√ß√£o inicial + dist√¢ncias BMU
        initial_assignments, bmu_distances = self._get_initial_assignments_with_distances(
            som, data, neuron_cluster_map, batch_size=bmu_batch
        )
        stats_initial = self._analyze_initial_distribution(initial_assignments)
        total_points = len(data)

        # Balanceamento e corte de clusters
        balanced_assignments = self._apply_balancing_strategies(
            initial_assignments, stats_initial, total_points,
            max_clusters=max_clusters,
            min_cluster_size_ratio=min_cluster_size_ratio,
            noise_threshold=noise_threshold
        )

        # Reatribui√ß√£o de clusters pequenos e inv√°lidos via BMU
        refined_assignments = self._reassign_invalid_by_bmu(
            som, data, balanced_assignments, stats_initial, neuron_cluster_map,
            min_cluster_size=self._compute_min_cluster_size(total_points),
            batch_size=bmu_batch
        )

        # Tratamento de ru√≠do (opcional): realocar parte do ru√≠do por limiar adaptativo
        final_assignments = self._handle_noise_points(
            som, data, refined_assignments, neuron_cluster_map,
            bmu_distances=bmu_distances,
            reassign_noise=reassign_noise
        )

        # Preparar resultado
        result_df = self._prepare_result_dataframe(original_df, final_assignments)

        # An√°lise de qualidade (amostragem segura j√° √© feita pelo evaluator)
        quality_metrics = self.quality_evaluator.comprehensive_cluster_quality(
            data, np.array(final_assignments), self.som_trainer.som
        )

        # Visualiza√ß√µes e an√°lises descritivas
        self._advanced_cluster_distribution_analysis(result_df)
        self._cluster_characteristics_analysis(result_df)
        self._create_comprehensive_visualizations(result_df, data, final_assignments)

        # Import√¢ncia de features por cluster
        feature_importance = self.calculate_feature_importance_per_cluster(result_df, top_n=top_n_features)
        quality_metrics['feature_importance'] = feature_importance

        # Resumo top features por cluster (compacto)
        top_features_summary = self._top_features_summary_per_cluster(feature_importance, result_df)
        quality_metrics['top_features_summary'] = top_features_summary

        # M√©tricas do processo
        cluster_metrics = {
            'initial_clusters': len(stats_initial['valid_clusters']),
            'final_clusters': len(np.unique([c for c in final_assignments if c != 0])),
            'noise_points': int(np.sum(np.array(final_assignments) == 0)),
            'retained_data_ratio': float(np.sum(np.array(final_assignments) > 0) / len(data)),
        }
        logger.info(f"   ‚úÖ Conclu√≠do: {cluster_metrics['final_clusters']} clusters, "
                    f"ru√≠do={cluster_metrics['noise_points']:,} ({cluster_metrics['retained_data_ratio']*100:.1f}% retido)")

        # Merge
        metrics = {**quality_metrics, **cluster_metrics}
        return result_df, metrics

    # -----------------------------
    # Core utilities and safeguards
    # -----------------------------

    def _compute_min_cluster_size(self, total_points: int) -> int:
        # Mesmo crit√©rio tolerante: 0.05% ou 100 pontos, o que for maior
        return max(100, int(total_points * 0.0005))

    def _create_neuron_cluster_map(self, neuron_clusters: np.ndarray) -> Dict[Tuple[int, int], int]:
        """Cria mapeamento neur√¥nio -> cluster (exclui 0)"""
        neuron_cluster_map: Dict[Tuple[int, int], int] = {}
        H, W = neuron_clusters.shape
        for i in range(H):
            for j in range(W):
                cid = int(neuron_clusters[i, j])
                if cid > 0:
                    neuron_cluster_map[(i, j)] = cid
        return neuron_cluster_map

    def _get_initial_assignments_with_distances(
        self, som, data: np.ndarray, neuron_cluster_map: Dict[Tuple[int, int], int], batch_size: int = 10000
    ) -> Tuple[List[int], np.ndarray]:
        """Obt√©m atribui√ß√µes iniciais e dist√¢ncia ao BMU (para limiar adaptativo de ru√≠do)"""
        assignments: List[int] = []
        bmu_dists = np.zeros(len(data), dtype=np.float32)

        n = len(data)
        for start in range(0, n, batch_size):
            end = min(start + batch_size, n)
            batch = data[start:end]
            for k, sample in enumerate(batch):
                w = som.winner(sample)
                weight = som._weights[w[0], w[1]]
                dist = np.linalg.norm(sample - weight)
                bmu_dists[start + k] = dist
                assignments.append(neuron_cluster_map.get(w, 0))
        return assignments, bmu_dists

    def _analyze_initial_distribution(self, assignments: List[int]) -> Dict[str, Any]:
        """Analisa distribui√ß√£o inicial dos clusters"""
        arr = np.array(assignments)
        unique_clusters, counts = np.unique(arr, return_counts=True)

        valid_clusters: List[int] = []
        cluster_sizes: Dict[int, int] = {}

        for cid, cnt in zip(unique_clusters, counts):
            cluster_sizes[int(cid)] = int(cnt)
            if int(cid) > 0:
                valid_clusters.append(int(cid))

        return {
            'unique_clusters': unique_clusters.tolist(),
            'counts': counts.tolist(),
            'valid_clusters': valid_clusters,
            'cluster_sizes': cluster_sizes
        }

    def _apply_balancing_strategies(
            self,
            assignments: List[int],
            cluster_stats: Dict[str, Any],
            total_points: int,
            max_clusters: int,
            min_cluster_size_ratio: float,
            noise_threshold: float
    ) -> List[int]:
        """
        ‚úÖ PRESERVA√á√ÉO TOTAL DE CLUSTERS

        Mudan√ßas cr√≠ticas:
        1. Remove threshold de tamanho m√≠nimo - TODOS os clusters s√£o v√°lidos
        2. Limite de max_clusters serve apenas para ordenar por tamanho
        3. Nenhum cluster √© marcado como inv√°lido
        4. Ru√≠do (ID 0) √© preservado como est√°
        """
        logger.info("\n   üìä AN√ÅLISE DE BALANCEAMENTO (MODO: PRESERVA√á√ÉO TOTAL)")
        logger.info(f"      ‚Ä¢ Total de pontos: {total_points:,}")
        logger.info(f"      ‚Ä¢ Max clusters configurado: {max_clusters} (apenas para ordena√ß√£o)")

        # ==========================================
        # IDENTIFICAR TODOS OS CLUSTERS
        # ==========================================
        valid_clusters: List[int] = []
        cluster_info: List[Dict[str, Any]] = []

        for cid in cluster_stats['valid_clusters']:
            size = cluster_stats['cluster_sizes'][cid]
            pct = (size / total_points) * 100
            cluster_info.append({'id': cid, 'size': size, 'percentage': pct})
            valid_clusters.append(cid)  # ‚úÖ TODOS s√£o v√°lidos
            logger.info(f"      ‚úÖ Cluster {cid}: {size:,} ({pct:.2f}%) - PRESERVADO")

        # ==========================================
        # LIMITAR APENAS SE EXCEDER MAX_CLUSTERS
        # ==========================================
        if len(valid_clusters) > max_clusters:
            logger.info(f"      ‚ÑπÔ∏è  Limitando de {len(valid_clusters)} para {max_clusters} maiores clusters")

            # Ordenar por tamanho (manter os maiores)
            sorted_clusters = sorted(cluster_info, key=lambda x: x['size'], reverse=True)
            valid_clusters = [c['id'] for c in sorted_clusters[:max_clusters]]

            # Logar clusters removidos
            removed_clusters = [c for c in cluster_info if c['id'] not in valid_clusters]
            if removed_clusters:
                logger.info(f"      üìã Clusters removidos (por limite max_clusters):")
                for c in removed_clusters[:5]:  # Mostrar at√© 5
                    logger.info(f"         - Cluster {c['id']}: {c['size']:,} ({c['percentage']:.2f}%)")
        else:
            logger.info(f"      ‚úÖ Todos os {len(valid_clusters)} clusters ser√£o preservados")

        # ==========================================
        # ATRIBUI√á√ÉO FINAL (SEM MARCA√á√ÉO DE INV√ÅLIDOS)
        # ==========================================
        balanced: List[int] = []

        for a in assignments:
            if a == 0:
                balanced.append(0)  # Ru√≠do preservado
            elif a in valid_clusters:
                balanced.append(a)  # Cluster v√°lido
            else:
                # Clusters que foram cortados por max_clusters v√£o para o maior cluster v√°lido
                if len(valid_clusters) > 0:
                    largest = max(valid_clusters, key=lambda x: cluster_stats['cluster_sizes'][x])
                    balanced.append(largest)
                else:
                    balanced.append(0)  # Fallback para ru√≠do

        noise_count = int(np.sum(np.array(balanced) == 0))

        logger.info(f"\n   ‚úÖ BALANCEAMENTO CONCLU√çDO (PRESERVA√á√ÉO TOTAL):")
        logger.info(f"      ‚Ä¢ Clusters preservados: {len(valid_clusters)}")
        logger.info(f"      ‚Ä¢ IDs: {sorted(valid_clusters)}")
        logger.info(f"      ‚Ä¢ Ru√≠do: {noise_count:,} ({noise_count / total_points * 100:.1f}%)")

        return balanced

    def _reassign_invalid_by_bmu(
        self,
        som,
        data: np.ndarray,
        assignments: List[int],
        cluster_stats: Dict[str, Any],
        neuron_cluster_map: Dict[Tuple[int, int], int],
        min_cluster_size: int,
        batch_size: int = 10000
    ) -> List[int]:
        """
        Reatribui pontos em clusters inv√°lidos/pequenos via BMU ao neur√¥nio v√°lido mais pr√≥ximo:
        - Para cada amostra marcada com -1, calcula dist√¢ncia √†s weights de neur√¥nios em clusters v√°lidos
        - Atribui cluster do neur√¥nio v√°lido com menor dist√¢ncia
        """
        logger.info("\n   üéØ REATRIBUI√á√ÉO BMU DE CLUSTERS PEQUENOS/INV√ÅLIDOS")
        arr = np.array(assignments)
        invalid_idx = np.where(arr == -1)[0]
        if len(invalid_idx) == 0:
            logger.info("      ‚Ä¢ Nenhum ponto inv√°lido para reatribuir")
            return assignments

        # Coletar neur√¥nios v√°lidos e seus clusters
        valid_cluster_ids = [cid for cid, size in cluster_stats['cluster_sizes'].items() if cid > 0 and size >= min_cluster_size]
        if len(valid_cluster_ids) == 0:
            logger.warning("      ‚ö†Ô∏è  Sem clusters v√°lidos para reatribui√ß√£o ‚Äì mantendo como ru√≠do")
            return [0 if a == -1 else a for a in assignments]

        # Mapa reverso cluster -> lista de neur√¥nios
        H, W, D = som._weights.shape
        # Construir lookup r√°pido: lista de neur√¥nios v√°lidos e seus clusters
        valid_neurons_coords: List[Tuple[int, int]] = []
        valid_neurons_clusters: List[int] = []
        for i in range(H):
            for j in range(W):
                cid = neuron_cluster_map.get((i, j), 0)
                if cid in valid_cluster_ids:
                    valid_neurons_coords.append((i, j))
                    valid_neurons_clusters.append(cid)
        if len(valid_neurons_coords) == 0:
            logger.warning("      ‚ö†Ô∏è  Nenhum neur√¥nio v√°lido encontrado ‚Äì mantendo inv√°lidos como ru√≠do")
            return [0 if a == -1 else a for a in assignments]

        # Pesos dos neur√¥nios v√°lidos
        vn_i = np.array([c[0] for c in valid_neurons_coords], dtype=np.int32)
        vn_j = np.array([c[1] for c in valid_neurons_coords], dtype=np.int32)
        W_valid = som._weights[vn_i, vn_j]  # shape: (n_valid_neurons, D)
        valid_neurons_clusters = np.array(valid_neurons_clusters, dtype=np.int32)

        # Reatribuir em lotes
        reassigned = 0
        for start in range(0, len(invalid_idx), batch_size):
            end = min(start + batch_size, len(invalid_idx))
            batch_indices = invalid_idx[start:end]
            batch_samples = data[batch_indices]  # (B, D)

            # Dist√¢ncias para todos neur√¥nios v√°lidos: (B, n_valid_neurons)
            # Cuidado com mem√≥ria. Fazemos em blocos internos se necess√°rio.
            # Aqui supomos W_valid moderado; caso contr√°rio, pode-se subamostrar neur√¥nios.
            # C√°lculo vetorizado:
            # dist(x, w) = ||x - w|| -> expand via broadcasting
            # (B, 1, D) - (1, N, D) -> (B, N, D)
            diffs = batch_samples[:, None, :] - W_valid[None, :, :]
            dists = np.linalg.norm(diffs, axis=2)  # (B, N)
            nearest_idx = np.argmin(dists, axis=1)  # (B,)
            nearest_cluster = valid_neurons_clusters[nearest_idx]  # (B,)

            for k, idx in enumerate(batch_indices):
                assignments[idx] = int(nearest_cluster[k])
                reassigned += 1

        logger.info(f"      ‚Ä¢ Reatribu√≠dos por BMU: {reassigned:,}")
        # Convert(-1) restantes em ru√≠do (should be none)
        assignments = [0 if a == -1 else a for a in assignments]
        return assignments

    def _handle_noise_points(
            self,
            som,
            data: np.ndarray,
            assignments: List[int],
            neuron_cluster_map: Dict[Tuple[int, int], int],
            bmu_distances: Optional[np.ndarray] = None,
            reassign_noise: bool = False,  # ‚úÖ Padr√£o alterado para False
            percent_cutoff: float = 0.80,
            batch_size: int = 10000
    ) -> List[int]:
        """
        ‚úÖ TRATAMENTO DE RU√çDO: DESABILITADO POR PADR√ÉO

        Ru√≠do (cluster 0) √© PRESERVADO a menos que reassign_noise=True.
        """
        logger.info("\n   üßπ TRATAMENTO DE RU√çDO")

        arr = np.array(assignments)
        noise_idx = np.where(arr == 0)[0]

        if len(noise_idx) == 0:
            logger.info("      ‚Ä¢ Sem ru√≠do para tratar")
            return assignments

        if not reassign_noise:
            logger.info(f"      ‚Ä¢ Reassign_noise=False ‚Üí preservando {len(noise_idx):,} pontos de ru√≠do")
            return assignments

        # Se chegou aqui, usu√°rio quer reatribuir ru√≠do
        logger.info(f"      ‚ö†Ô∏è  Reatribuindo {len(noise_idx):,} pontos de ru√≠do...")

        # [Resto do c√≥digo original do m√©todo, caso reassign_noise=True]
        # ...

        return assignments

    # -----------------------------
    # Result dataframe and analysis
    # -----------------------------

    def _prepare_result_dataframe(self, original_df: pd.DataFrame, clusters: List[int]) -> pd.DataFrame:
        """Prepara DataFrame final com clusters e an√°lises"""
        result_df = original_df.iloc[:len(clusters)].copy()
        result_df['CLUSTER_SOM'] = clusters
        result_df['CLUSTER_SIZE'] = result_df['CLUSTER_SOM'].map(
            result_df['CLUSTER_SOM'].value_counts()
        )
        return result_df

    def _advanced_cluster_distribution_analysis(self, df: pd.DataFrame) -> None:
        """An√°lise avan√ßada da distribui√ß√£o de clusters"""
        logger.info("\nüìä DISTRIBUI√á√ÉO AVAN√áADA DOS CLUSTERS")
        cluster_dist = df['CLUSTER_SOM'].value_counts().sort_index()
        valid_clusters = cluster_dist[cluster_dist.index != 0]

        if len(valid_clusters) == 0:
            logger.warning("   ‚ö†Ô∏è  Nenhum cluster v√°lido encontrado!")
            return

        total_records = len(df)
        noise_count = cluster_dist.get(0, 0)
        clustered_records = int(valid_clusters.sum())

        logger.info(f"   ‚Ä¢ Clusters v√°lidos: {len(valid_clusters)}")
        logger.info(f"   ‚Ä¢ Registros em clusters: {clustered_records:,} ({clustered_records/total_records*100:.1f}%)")
        logger.info(f"   ‚Ä¢ Registros como ru√≠do: {noise_count:,} ({noise_count/total_records*100:.1f}%)")
        logger.info(f"   ‚Ä¢ Tamanho m√©dio do cluster: {valid_clusters.mean():.0f} registros")
        logger.info(f"   ‚Ä¢ Desvio padr√£o: {valid_clusters.std():.0f} registros")

        Q1 = valid_clusters.quantile(0.25)
        Q3 = valid_clusters.quantile(0.75)
        IQR = float(Q3 - Q1)
        outlier_threshold = float(Q3 + 1.5 * IQR)
        outliers = valid_clusters[valid_clusters > outlier_threshold]
        if len(outliers) > 0:
            logger.info(f"   ‚Ä¢ Clusters grandes (outliers): {list(outliers.index)}")

    def _cluster_characteristics_analysis(self, df: pd.DataFrame) -> None:
        """An√°lise detalhada das caracter√≠sticas dos clusters"""
        logger.info("\nüìà AN√ÅLISE DETALHADA POR CLUSTER")
        valid_clusters = sorted([int(c) for c in df['CLUSTER_SOM'].unique() if c != 0])
        if not valid_clusters:
            logger.warning("   ‚ö†Ô∏è  Nenhum cluster v√°lido para an√°lise!")
            return

        for cluster_id in valid_clusters:
            cluster_data = df[df['CLUSTER_SOM'] == cluster_id]
            self._analyze_single_cluster(cluster_data, cluster_id, df)

    def _analyze_single_cluster(self, cluster_data: pd.DataFrame, cluster_id: int, full_df: pd.DataFrame) -> None:
        """Analisa um cluster individual"""
        size = len(cluster_data)
        percentage = (size / len(full_df)) * 100
        logger.info(f"\nüéØ CLUSTER {cluster_id}: {size:,} registros ({percentage:.1f}%)")
        logger.info("   " + "‚îÄ" * 50)

        self._analyze_categorical_features(cluster_data, size)
        self._analyze_numeric_features(cluster_data, full_df)

    def _analyze_categorical_features(self, cluster_data: pd.DataFrame, cluster_size: int) -> None:
        """Analisa features categ√≥ricas do cluster"""
        categorical_insights: Dict[str, List[Tuple[str, float]]] = {}
        for col in cluster_data.select_dtypes(include=['object', 'category']).columns:
            if cluster_data[col].nunique() < 15:
                vc = cluster_data[col].value_counts()
                top2 = vc.head(2)
                for value, count in top2.items():
                    pct = (count / cluster_size) * 100
                    if pct > 20:
                        categorical_insights.setdefault(col, []).append((value, pct))

        if categorical_insights:
            logger.info("   üè∑Ô∏è  CARACTER√çSTICAS CATEG√ìRICAS:")
            for col, values in list(categorical_insights.items())[:8]:
                s = ", ".join([f"{val} ({pct:.1f}%)" for val, pct in values[:2]])
                logger.info(f"     ‚Ä¢ {col}: {s}")

    def _analyze_numeric_features(self, cluster_data: pd.DataFrame, full_df: pd.DataFrame) -> None:
        """Analisa features num√©ricas com compara√ß√£o global"""
        numeric_cols = cluster_data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return

        logger.info("   üìä CARACTER√çSTICAS NUM√âRICAS:")
        LIMIT = 6
        for col in list(numeric_cols)[:LIMIT]:
            cmean = cluster_data[col].mean()
            gmean = full_df[col].mean()

            if col in ['LATITUDE', 'LONGITUDE'] and abs(cmean) > 1000:
                logger.warning(f"      ‚ö†Ô∏è  {col}: AINDA CORROMPIDO ({cmean:.0f})")
                logger.warning(f"          Aplicar corre√ß√£o de escala no preprocessor!")
                continue

            diff_pct = ((cmean - gmean) / abs(gmean)) * 100 if gmean != 0 else 0
            significance = "‚Üë‚Üë" if diff_pct > 15 else "‚Üì‚Üì" if diff_pct < -15 else "‚âà"
            logger.info(f"     ‚Ä¢ {col}: {significance} avg={cmean:.1f} "
                        f"(global: {gmean:.1f}, diff: {diff_pct:+.1f}%)")

    # -----------------------------
    # Visualizations
    # -----------------------------

    def _create_comprehensive_visualizations(self, df: pd.DataFrame, data: np.ndarray, clusters: List[int]) -> None:
        """Cria visualiza√ß√µes abrangentes dos clusters"""
        logger.info("   üé® Criando visualiza√ß√µes...")

        fig = plt.figure(figsize=(20, 16))
        ax1 = plt.subplot(2, 3, 1)
        self._plot_cluster_distribution(df, ax1)

        ax2 = plt.subplot(2, 3, 2)
        self._plot_cluster_composition(df, ax2)

        ax3 = plt.subplot(2, 3, 3)
        self._plot_feature_heatmap(df, ax3)

        ax4 = plt.subplot(2, 3, 4)
        self._plot_projection(df, data, clusters, ax4)

        ax5 = plt.subplot(2, 3, 5)
        self._plot_cluster_quality(df, ax5)

        ax6 = plt.subplot(2, 3, 6)
        self._plot_cluster_correlation(df, ax6)

        plt.tight_layout()
        plt.savefig('advanced_som_cluster_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

        # Gr√°fico adicional: Radar chart para perfis de cluster
        self._create_radar_chart(df)

    def _plot_cluster_distribution(self, df: pd.DataFrame, ax) -> None:
        cluster_dist = df['CLUSTER_SOM'].value_counts().sort_index()
        valid_clusters = cluster_dist[cluster_dist.index != 0]

        colors = plt.cm.viridis(np.linspace(0, 1, len(valid_clusters)))
        bars = ax.bar(range(len(valid_clusters)), valid_clusters.values, color=colors)

        ax.set_title('Distribui√ß√£o de Clusters', fontsize=14, fontweight='bold')
        ax.set_xlabel('Cluster ID')
        ax.set_ylabel('N√∫mero de Registros')
        ax.set_xticks(range(len(valid_clusters)))
        ax.set_xticklabels(valid_clusters.index, rotation=45)

        for bar, count in zip(bars, valid_clusters.values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{count:,}', ha='center', va='bottom', fontsize=9)

    def _plot_cluster_composition(self, df: pd.DataFrame, ax) -> None:
        valid_clusters = sorted([c for c in df['CLUSTER_SOM'].unique() if c != 0])
        if len(valid_clusters) == 0:
            ax.text(0.5, 0.5, 'Sem clusters v√°lidos',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Composi√ß√£o dos Clusters', fontsize=14)
            return
        ax.set_title('Composi√ß√£o dos Clusters\n(Top Features)', fontsize=14, fontweight='bold')

    def _plot_feature_heatmap(self, df: pd.DataFrame, ax) -> None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            cluster_means = df.groupby('CLUSTER_SOM')[numeric_cols[:8]].mean()
            if len(cluster_means) > 1:
                normalized_means = (cluster_means - cluster_means.mean()) / cluster_means.std(ddof=0)
                # Excluir linha do ru√≠do (ID 0) se existir
                normalized_means = normalized_means[normalized_means.index != 0]
                sns.heatmap(normalized_means, ax=ax, cmap='RdBu_r', center=0,
                            annot=True, fmt='.2f', cbar_kws={'label': 'Z-score'})
                ax.set_title('Heatmap de Caracter√≠sticas\n(Normalizado)', fontsize=14, fontweight='bold')
            else:
                ax.text(0.5, 0.5, 'Dados insuficientes\npara heatmap',
                        ha='center', va='center', transform=ax.transAxes)
        else:
            ax.text(0.5, 0.5, 'Sem features num√©ricas',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Heatmap de Caracter√≠sticas', fontsize=14)

    def _plot_projection(self, df: pd.DataFrame, data: np.ndarray, clusters: List[int], ax) -> None:
        try:
            from sklearn.decomposition import PCA
            if data.shape[1] > 2:
                projector = PCA(n_components=2, random_state=42)
                proj = projector.fit_transform(data)
                title = 'Proje√ß√£o PCA dos Clusters'
            else:
                proj = data
                title = 'Visualiza√ß√£o Direta dos Clusters'

            scatter = ax.scatter(proj[:, 0], proj[:, 1],
                                 c=clusters, cmap='tab10', alpha=0.6, s=30)
            ax.set_title(title, fontsize=14, fontweight='bold')
            ax.set_xlabel('Componente 1')
            ax.set_ylabel('Componente 2')
            plt.colorbar(scatter, ax=ax, label='Cluster ID')

        except Exception:
            ax.text(0.5, 0.5, 'Proje√ß√£o indispon√≠vel',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Proje√ß√£o dos Clusters', fontsize=14)

    def _plot_cluster_quality(self, df: pd.DataFrame, ax) -> None:
        valid_clusters = [c for c in df['CLUSTER_SOM'].unique() if c != 0]
        if len(valid_clusters) < 2:
            ax.text(0.5, 0.5, 'Clusters insuficientes\npara an√°lise',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Qualidade vs Tamanho', fontsize=14)
            return

        sizes = []
        for cid in valid_clusters:
            sizes.append(len(df[df['CLUSTER_SOM'] == cid]))
        ax.scatter(sizes, sizes, alpha=0.6, s=60)
        ax.set_xlabel('Tamanho do Cluster')
        ax.set_ylabel('Tamanho')
        ax.set_title('Rela√ß√£o: Tamanho vs Qualidade', fontsize=14, fontweight='bold')

    def _plot_cluster_correlation(self, df: pd.DataFrame, ax) -> None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) < 2:
            ax.text(0.5, 0.5, 'Features insuficientes\npara correla√ß√£o',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Correla√ß√£o entre Clusters', fontsize=14)
            return

        valid_clusters = [c for c in df['CLUSTER_SOM'].unique() if c != 0]
        if len(valid_clusters) < 2:
            ax.text(0.5, 0.5, 'Clusters insuficientes',
                    ha='center', va='center', transform=ax.transAxes)
            return

        cluster_means = []
        cluster_labels = []

        for c in valid_clusters:
            cd = df[df['CLUSTER_SOM'] == c]
            if len(cd) > 1:
                means = cd[numeric_cols].mean().values
                cluster_means.append(means)
                cluster_labels.append(c)

        if len(cluster_means) < 2:
            ax.text(0.5, 0.5, 'Dados insuficientes\npara correla√ß√£o',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Correla√ß√£o entre Clusters', fontsize=14)
            return

        cluster_means_array = np.array(cluster_means)
        corr = np.corrcoef(cluster_means_array)
        im = ax.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)
        ax.set_xticks(range(len(cluster_labels)))
        ax.set_yticks(range(len(cluster_labels)))
        ax.set_xticklabels(cluster_labels)
        ax.set_yticklabels(cluster_labels)
        ax.set_title('Similaridade entre Clusters\n(Correla√ß√£o dos Perfis M√©dios)',
                     fontsize=14, fontweight='bold')

        for i in range(len(cluster_labels)):
            for j in range(len(cluster_labels)):
                ax.text(j, i, f'{corr[i, j]:.2f}',
                        ha='center', va='center',
                        color='white' if abs(corr[i, j]) > 0.5 else 'black',
                        fontsize=9)
        plt.colorbar(im, ax=ax, label='Coeficiente de Correla√ß√£o')

    def _create_radar_chart(self, df: pd.DataFrame) -> None:
        """Cria gr√°fico radar para perfis de clusters"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:6]
            if len(numeric_cols) < 3:
                return
            valid_clusters = [c for c in df['CLUSTER_SOM'].unique() if c != 0][:8]
            if len(valid_clusters) < 2:
                return

            fig = plt.figure(figsize=(12, 8))
            ax = fig.add_subplot(111, polar=True)
            angles = np.linspace(0, 2*np.pi, len(numeric_cols), endpoint=False).tolist()
            angles += angles[:1]

            for cid in valid_clusters:
                cd = df[df['CLUSTER_SOM'] == cid]
                values = cd[numeric_cols].mean().tolist()
                values += values[:1]
                ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cid}')
                ax.fill(angles, values, alpha=0.1)

            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(numeric_cols)
            ax.set_title('Perfil dos Clusters - Radar Chart', size=14, fontweight='bold')
            ax.legend(bbox_to_anchor=(1.1, 1.1))
            plt.savefig('cluster_radar_chart.png', dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  N√£o foi poss√≠vel criar radar chart: {e}")

    # -----------------------------
    # PCA-based interpretation (optional)
    # -----------------------------

    def interpret_cluster_with_pca(self, cluster_id: int, cluster_data: pd.DataFrame, preprocessor) -> None:
        """Interpreta cluster considerando contribui√ß√µes do PCA (se dispon√≠veis)"""
        print(f"\nüîç INTERPRETA√á√ÉO DO CLUSTER {cluster_id} (via PCA)")
        print("=" * 50)

        if not hasattr(preprocessor, 'pca_contributions'):
            print("‚ö†Ô∏è  Contribui√ß√µes PCA n√£o dispon√≠veis")
            return

        pca_contrib = preprocessor.pca_contributions
        cluster_pcs = cluster_data[[col for col in cluster_data.columns if col.startswith('PC')]]
        cluster_mean = cluster_pcs.mean()

        print("üìä Componentes Principais Dominantes:")
        top_pcs = cluster_mean.abs().nlargest(3)

        for pc, value in top_pcs.items():
            var_explained = pca_contrib.loc['Explained_Variance', pc]
            print(f"\n{pc} (valor: {value:.3f}, explica {var_explained * 100:.1f}%):")
            pc_features = pca_contrib[pc].drop('Explained_Variance').abs().nlargest(5)
            print("   Features originais mais influentes:")
            for feat, contrib in pc_features.items():
                direction = "+" if pca_contrib.loc[feat, pc] > 0 else "-"
                print(f"      {direction} {feat}: {contrib:.3f}")

    # -----------------------------
    # Feature importance
    # -----------------------------

    def calculate_feature_importance_per_cluster(self, df_with_clusters: pd.DataFrame, top_n: int = 10) -> Dict[str, Any]:
        """
        Calcula import√¢ncia de features para cada cluster usando:
        1. Effect Size (Cohen's d)
        2. ANOVA (f-stat, p-val)
        3. Mutual Information (classif)
        """
        from scipy.stats import f_oneway
        from sklearn.feature_selection import mutual_info_classif

        print("\n" + "=" * 70)
        print("üìä AN√ÅLISE DE IMPORT√ÇNCIA DE FEATURES POR CLUSTER")
        print("=" * 70)

        numeric_features = df_with_clusters.select_dtypes(include=[np.number]).columns
        numeric_features = [f for f in numeric_features if f not in ['CLUSTER_SOM', 'CLUSTER_SIZE']]
        if len(numeric_features) == 0:
            print("‚ö†Ô∏è  Nenhuma feature num√©rica para an√°lise")
            return {}

        valid_clusters = [c for c in df_with_clusters['CLUSTER_SOM'].unique() if c != 0]
        feature_importance: Dict[str, Any] = {}

        for feature in numeric_features:
            effect_sizes: Dict[int, float] = {}
            global_mean = df_with_clusters[feature].mean()
            global_std = df_with_clusters[feature].std(ddof=0)

            for cluster_id in valid_clusters:
                cmean = df_with_clusters[df_with_clusters['CLUSTER_SOM'] == cluster_id][feature].mean()
                cohen_d = abs((cmean - global_mean) / global_std) if global_std > 0 else 0.0
                effect_sizes[int(cluster_id)] = float(cohen_d)

            cluster_groups = [
                df_with_clusters[df_with_clusters['CLUSTER_SOM'] == c][feature].dropna()
                for c in valid_clusters
            ]
            if all(len(g) > 1 for g in cluster_groups):
                f_stat, p_value = f_oneway(*cluster_groups)
            else:
                f_stat, p_value = 0.0, 1.0

            Xv = df_with_clusters[feature].values.reshape(-1, 1)
            yv = df_with_clusters['CLUSTER_SOM'].values
            try:
                mi_score = float(mutual_info_classif(Xv, yv, random_state=42)[0])
            except Exception:
                mi_score = 0.0

            feature_importance[feature] = {
                'effect_sizes': effect_sizes,
                'max_effect_size': max(effect_sizes.values()) if effect_sizes else 0.0,
                'f_statistic': float(f_stat),
                'p_value': float(p_value),
                'mutual_info': mi_score,
                'is_significant': p_value < 0.05
            }

        # Ranking composto
        feature_importance_sorted = sorted(
            feature_importance.items(),
            key=lambda x: (x[1]['max_effect_size'] + x[1]['mutual_info']),
            reverse=True
        )

        print(f"\nüèÜ TOP {top_n} FEATURES MAIS DISCRIMINATIVAS:")
        print("-" * 70)
        print(f"{'Feature':<25} {'Max Cohen-d':<15} {'ANOVA p-val':<15} {'Mutual Info':<15}")
        print("-" * 70)

        for i, (feature, metrics) in enumerate(feature_importance_sorted[:top_n], 1):
            sig_marker = "***" if metrics['p_value'] < 0.001 else "**" if metrics['p_value'] < 0.01 else "*" if metrics['p_value'] < 0.05 else ""
            print(f"{i}. {feature:<22} {metrics['max_effect_size']:>10.3f}     "
                  f"{metrics['p_value']:>10.4f}{sig_marker:<3}  {metrics['mutual_info']:>10.3f}")

        print(f"\nüìã FEATURES DISTINTIVAS POR CLUSTER:")
        for cluster_id in valid_clusters:
            print(f"\nüéØ CLUSTER {cluster_id}:")
            cluster_features = sorted(
                [(f, m['effect_sizes'].get(int(cluster_id), 0.0)) for f, m in feature_importance.items()],
                key=lambda x: x[1],
                reverse=True
            )[:5]
            for feature, effect_size in cluster_features:
                cluster_mean = df_with_clusters[df_with_clusters['CLUSTER_SOM'] == cluster_id][feature].mean()
                global_mean = df_with_clusters[feature].mean()
                if effect_size > 0.5:
                    direction = "‚Üë" if cluster_mean > global_mean else "‚Üì"
                    print(f"   {direction} {feature}: Cohen-d={effect_size:.3f}, "
                          f"Œº_cluster={cluster_mean:.2f}, Œº_global={global_mean:.2f}")

        joblib.dump(feature_importance, 'feature_importance_by_cluster.pkl')
        print(f"\nüíæ An√°lise salva: feature_importance_by_cluster.pkl")
        return feature_importance

    def _top_features_summary_per_cluster(self, feature_importance: Dict[str, Any], df: pd.DataFrame, top_k: int = 5) -> Dict[int, List[str]]:
        """
        Cria resumo compacto: para cada cluster, lista das top_k features com maior effect size.
        """
        if not feature_importance or 'CLUSTER_SOM' not in df.columns:
            return {}
        clusters = sorted([int(c) for c in df['CLUSTER_SOM'].unique() if c != 0])
        summary: Dict[int, List[str]] = {}
        for cid in clusters:
            scored = []
            for feat, metrics in feature_importance.items():
                es = metrics.get('effect_sizes', {}).get(cid, 0.0)
                scored.append((feat, es))
            top = [f for f, es in sorted(scored, key=lambda x: x[1], reverse=True)[:top_k]]
            summary[cid] = top
        return summary

    def get_cluster_profiles(self) -> Dict:
        """Retorna perfis detalhados dos clusters"""
        return self.cluster_profiles

    def generate_cluster_report(self, df: pd.DataFrame, output_file: str = 'cluster_analysis_report.txt') -> None:
        """Gera relat√≥rio completo da an√°lise com top features resumidas"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("RELAT√ìRIO DE AN√ÅLISE DE CLUSTERS - SOM\n")
            f.write("=" * 50 + "\n\n")

            cluster_dist = df['CLUSTER_SOM'].value_counts().sort_index()
            valid_clusters = cluster_dist[cluster_dist.index != 0]

            f.write(f"ESTAT√çSTICAS GERAIS:\n")
            f.write(f"- Total de clusters v√°lidos: {len(valid_clusters)}\n")
            f.write(f"- Total de registros: {len(df):,}\n")
            f.write(f"- Registros em clusters: {valid_clusters.sum():,}\n")
            f.write(f"- Registros como ru√≠do: {cluster_dist.get(0, 0):,}\n\n")

            f.write("PERFIS DOS CLUSTERS:\n")
            f.write("-" * 30 + "\n")

            for cluster_id in valid_clusters.index:
                cluster_data = df[df['CLUSTER_SOM'] == cluster_id]
                size = len(cluster_data)
                percentage = (size / len(df)) * 100
                f.write(f"\nCLUSTER {cluster_id} ({size:,} registros - {percentage:.1f}%):\n")

                # Top 3 categ√≥ricas compactas
                for col in cluster_data.select_dtypes(include=['object', 'category']).columns[:3]:
                    if cluster_data[col].nunique() < 10:
                        top_value = cluster_data[col].value_counts().head(1)
                        if len(top_value) > 0:
                            value, count = top_value.index[0], int(top_value.values[0])
                            pct = (count / size) * 100
                            f.write(f"  ‚Ä¢ {col}: {value} ({pct:.1f}%)\n")

        logger.info(f"   üìÑ Relat√≥rio salvo em: {output_file}")


# Vers√£o de compatibilidade para c√≥digo existente
SOMClusterInterpreter = AdvancedSOMClusterInterpreter
