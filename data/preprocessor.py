"""
M√≥dulo de pr√©-processamento de dados - VERS√ÉO OTIMIZADA
Corre√ß√µes cr√≠ticas implementadas:
1. Limpeza robusta de outliers geogr√°ficos
2. Normaliza√ß√£o resistente a ru√≠do (RobustScaler)
3. Redu√ß√£o de dimensionalidade com PCA
4. Detec√ß√£o avan√ßada de outliers (LOF + IsolationForest)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import LocalOutlierFactor
from sklearn.ensemble import IsolationForest
from scipy import sparse
import joblib
import os
import logging
from typing import List, Tuple

logger = logging.getLogger(__name__)

class AdvancedDataPreprocessor:
    """Pr√©-processamento avan√ßado com detec√ß√£o robusta de outliers"""

    def __init__(self, pca_variance=0.90):
        # ‚úÖ MUDAN√áA 1: RobustScaler ao inv√©s de StandardScaler
        self.scaler = RobustScaler()  # Resistente a outliers
        self.encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')
        self.pca = PCA(n_components=pca_variance, random_state=42)
        self.feature_info = {}
        self.categorical_mappings = {}
        self.outlier_stats = {}

    def load_and_analyze_data(self, csv_path='SPSafe_2022.csv', sample_frac=None):
        """Carrega e analisa dados com relat√≥rio detalhado"""
        print("üìä CARREGAMENTO E AN√ÅLISE EXPLORAT√ìRIA DE DADOS")
        print("=" * 50)

        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Arquivo {csv_path} n√£o encontrado!")

        try:
            df = pd.read_csv(csv_path, sep=';', encoding='utf-8', low_memory=False)
        except UnicodeDecodeError:
            df = pd.read_csv(csv_path, sep=';', encoding='latin-1', low_memory=False)

        original_size = len(df)
        if sample_frac and sample_frac < 1.0:
            df = df.sample(frac=sample_frac, random_state=42)
            print(f"‚úÖ Dataset amostrado: {len(df):,} registros ({sample_frac * 100:.1f}% do original)")

        print(f"üìà Shape do dataset: {df.shape}")

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object']).columns

        print(f"   ‚Ä¢ Num√©ricas: {len(numeric_cols)} colunas")
        print(f"   ‚Ä¢ Categ√≥ricas: {len(categorical_cols)} colunas")

        print(f"\nüìã Estat√≠sticas b√°sicas:")
        print(f"   ‚Ä¢ Registros totais: {len(df):,}")
        print(f"   ‚Ä¢ Valores missing: {df.isnull().sum().sum():,}")
        print(f"   ‚Ä¢ Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024 ** 2:.1f} MB")

        return df

    def clean_geographic_outliers(self, df):
        """
        ‚úÖ CORRE√á√ÉO CR√çTICA 1: Limpeza robusta de coordenadas geogr√°ficas
        Remove valores imposs√≠veis que estavam destruindo o SOM
        """
        print("\nüó∫Ô∏è  LIMPEZA CR√çTICA DE COORDENADAS GEOGR√ÅFICAS")
        print("=" * 50)
        
        initial_size = len(df)
        
        # Limites v√°lidos para S√£o Paulo
        LAT_MIN, LAT_MAX = -25.0, -19.0
        LON_MIN, LON_MAX = -48.0, -44.0
        
        if 'LATITUDE' in df.columns and 'LONGITUDE' in df.columns:
            df['LATITUDE'] = pd.to_numeric(df['LATITUDE'], errors='coerce')
            df['LONGITUDE'] = pd.to_numeric(df['LONGITUDE'], errors='coerce')
            
            # Log dos outliers antes da limpeza
            lat_outliers = ((df['LATITUDE'] < LAT_MIN) | (df['LATITUDE'] > LAT_MAX)).sum()
            lon_outliers = ((df['LONGITUDE'] < LON_MIN) | (df['LONGITUDE'] > LON_MAX)).sum()
            
            print(f"‚ö†Ô∏è  OUTLIERS DETECTADOS:")
            print(f"   ‚Ä¢ Latitudes inv√°lidas: {lat_outliers:,} ({lat_outliers/len(df)*100:.2f}%)")
            print(f"   ‚Ä¢ Longitudes inv√°lidas: {lon_outliers:,} ({lon_outliers/len(df)*100:.2f}%)")
            
            # Estrat√©gia 1: Remover outliers extremos
            mask_valid = (
                (df['LATITUDE'].between(LAT_MIN, LAT_MAX)) &
                (df['LONGITUDE'].between(LON_MIN, LON_MAX))
            )
            
            df_clean = df[mask_valid].copy()
            removed = initial_size - len(df_clean)
            
            print(f"\n‚úÖ LIMPEZA CONCLU√çDA:")
            print(f"   ‚Ä¢ Registros removidos: {removed:,} ({removed/initial_size*100:.2f}%)")
            print(f"   ‚Ä¢ Registros mantidos: {len(df_clean):,}")
            
            # Estat√≠sticas ap√≥s limpeza
            if len(df_clean) > 0:
                print(f"\nüìä COORDENADAS AP√ìS LIMPEZA:")
                print(f"   ‚Ä¢ Latitude: [{df_clean['LATITUDE'].min():.4f}, {df_clean['LATITUDE'].max():.4f}]")
                print(f"   ‚Ä¢ Longitude: [{df_clean['LONGITUDE'].min():.4f}, {df_clean['LONGITUDE'].max():.4f}]")
            
            self.outlier_stats['geographic_outliers_removed'] = removed
            
            return df_clean
        
        return df

    def detect_spatial_outliers(self, df):
        """
        ‚úÖ CORRE√á√ÉO CR√çTICA 2: Detec√ß√£o de outliers espaciais usando LOF
        Implementa t√©cnica validada pela literatura para dados criminais
        """
        print("\nüîç DETEC√á√ÉO AVAN√áADA DE OUTLIERS ESPACIAIS (LOF)")
        print("=" * 55)
        
        if 'LATITUDE' not in df.columns or 'LONGITUDE' not in df.columns:
            return df
        
        # Remover NaN antes do LOF
        spatial_mask = df['LATITUDE'].notna() & df['LONGITUDE'].notna()
        df_spatial = df[spatial_mask].copy()
        
        if len(df_spatial) < 100:
            print("‚ö†Ô∏è  Dados insuficientes para detec√ß√£o LOF")
            return df
        
        # Aplicar LOF nas coordenadas geogr√°ficas
        spatial_features = df_spatial[['LATITUDE', 'LONGITUDE']].values
        
        # Ajustar n_neighbors baseado no tamanho do dataset
        n_neighbors = min(20, len(df_spatial) // 100)
        contamination = 0.05  # Espera-se 5% de outliers
        
        print(f"   ‚Ä¢ Aplicando LOF com n_neighbors={n_neighbors}")
        print(f"   ‚Ä¢ Contamina√ß√£o esperada: {contamination*100:.1f}%")
        
        try:
            lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)
            outlier_labels = lof.fit_predict(spatial_features)
            
            # -1 = outlier, 1 = inlier
            n_outliers = (outlier_labels == -1).sum()
            
            print(f"\n‚úÖ OUTLIERS ESPACIAIS DETECTADOS:")
            print(f"   ‚Ä¢ Outliers encontrados: {n_outliers:,} ({n_outliers/len(df_spatial)*100:.2f}%)")
            
            # Adicionar flag de outlier ao dataframe original
            df_spatial['SPATIAL_OUTLIER'] = outlier_labels == -1
            
            # Mesclar de volta ao dataframe original
            df = df.merge(
                df_spatial[['SPATIAL_OUTLIER']], 
                left_index=True, 
                right_index=True, 
                how='left'
            )
            df['SPATIAL_OUTLIER'] = df['SPATIAL_OUTLIER'].fillna(False)
            
            # Remover outliers espaciais
            df_clean = df[~df['SPATIAL_OUTLIER']].copy()
            df_clean = df_clean.drop('SPATIAL_OUTLIER', axis=1)
            
            print(f"   ‚Ä¢ Registros mantidos: {len(df_clean):,}")
            
            self.outlier_stats['spatial_outliers_removed'] = n_outliers
            
            return df_clean
            
        except Exception as e:
            logger.warning(f"Erro no LOF: {e}. Pulando detec√ß√£o espacial.")
            return df

    def detect_multivariate_outliers(self, X_numeric):
        """
        ‚úÖ CORRE√á√ÉO CR√çTICA 3: Detec√ß√£o multivariada de outliers
        Usa Isolation Forest para detectar outliers em todas as features num√©ricas
        """
        print("\nüå≤ DETEC√á√ÉO MULTIVARIADA DE OUTLIERS (ISOLATION FOREST)")
        print("=" * 60)
        
        if X_numeric.shape[1] == 0:
            return np.ones(X_numeric.shape[0], dtype=bool)
        
        contamination = 0.05  # 5% esperado de outliers
        
        print(f"   ‚Ä¢ Features num√©ricas analisadas: {X_numeric.shape[1]}")
        print(f"   ‚Ä¢ Contamina√ß√£o esperada: {contamination*100:.1f}%")
        
        try:
            iso_forest = IsolationForest(
                contamination=contamination,
                random_state=42,
                n_jobs=-1
            )
            outlier_labels = iso_forest.fit_predict(X_numeric)
            
            # -1 = outlier, 1 = inlier
            inlier_mask = outlier_labels == 1
            n_outliers = (~inlier_mask).sum()
            
            print(f"\n‚úÖ OUTLIERS MULTIVARIADOS DETECTADOS:")
            print(f"   ‚Ä¢ Outliers encontrados: {n_outliers:,} ({n_outliers/len(X_numeric)*100:.2f}%)")
            print(f"   ‚Ä¢ Registros mantidos: {inlier_mask.sum():,}")
            
            self.outlier_stats['multivariate_outliers_removed'] = n_outliers
            
            return inlier_mask
            
        except Exception as e:
            logger.warning(f"Erro no Isolation Forest: {e}")
            return np.ones(X_numeric.shape[0], dtype=bool)

    def create_eda_visualizations(self, df):
        """Cria visualiza√ß√µes de an√°lise explorat√≥ria"""
        print("\nüé® CRIANDO VISUALIZA√á√ïES EXPLORAT√ìRIAS...")

        plt.figure(figsize=(10, 6))
        dtype_counts = df.dtypes.value_counts()
        plt.pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')
        plt.title('Distribui√ß√£o de Tipos de Dados')
        plt.savefig('eda_data_types.png', dpi=300, bbox_inches='tight')
        plt.close()

        missing_data = df.isnull().sum().sort_values(ascending=False).head(20)
        if len(missing_data) > 0:
            plt.figure(figsize=(12, 8))
            missing_data.plot(kind='barh')
            plt.title('Top 20 Colunas com Valores Missing')
            plt.xlabel('N√∫mero de Valores Missing')
            plt.tight_layout()
            plt.savefig('eda_missing_values.png', dpi=300, bbox_inches='tight')
            plt.close()

        print("‚úÖ Visualiza√ß√µes explorat√≥rias salvas")

    def enhanced_feature_engineering(self, df):
        """Engenharia de features com an√°lise detalhada"""
        print("\nüîß ENGENHARIA DE FEATURES AVAN√áADA")
        print("=" * 40)

        df = df.copy()

        temporal_features = []
        if 'DATA_OCORRENCIA' in df.columns:
            df['DATA_OCORRENCIA'] = pd.to_datetime(df['DATA_OCORRENCIA'], errors='coerce')
            df['DIA_SEMANA'] = df['DATA_OCORRENCIA'].dt.day_name()
            df['MES'] = df['DATA_OCORRENCIA'].dt.month_name()
            df['ANO'] = df['DATA_OCORRENCIA'].dt.year
            df['FIM_SEMANA'] = df['DATA_OCORRENCIA'].dt.weekday >= 5
            temporal_features.extend(['DIA_SEMANA', 'MES', 'ANO', 'FIM_SEMANA'])

        if 'HORA_OCORRENCIA' in df.columns:
            def parse_hour_detailed(h):
                try:
                    s = str(h).strip().replace('h', ':').replace('.', ':')
                    if ':' in s:
                        return int(s.split(':')[0])
                    elif s.isdigit():
                        return int(s[:2]) if len(s) > 2 else int(s)
                except:
                    return np.nan
                return np.nan

            df['HORA'] = df['HORA_OCORRENCIA'].apply(parse_hour_detailed)

            bins = [-1, 5, 9, 12, 15, 18, 21, 24]
            labels = ['Madrugada', 'Manh√£ Cedo', 'Manh√£', 'Tarde Cedo', 'Tarde', 'Noite', 'Noite Tardia']
            df['PERIODO_DIA'] = pd.cut(df['HORA'], bins=bins, labels=labels).astype(str)
            temporal_features.extend(['HORA', 'PERIODO_DIA'])

        geographic_features = []
        if all(col in df.columns for col in ['LATITUDE', 'LONGITUDE']):
            df['TEM_COORDENADAS'] = df['LATITUDE'].notna() & df['LONGITUDE'].notna()
            geographic_features.extend(['LATITUDE', 'LONGITUDE', 'TEM_COORDENADAS'])

        demographic_features = []
        if 'IDADE_PESSOA' in df.columns:
            df['IDADE_PESSOA'] = pd.to_numeric(df['IDADE_PESSOA'], errors='coerce')
            bins = [0, 18, 30, 45, 60, 100, 200]
            labels = ['0-18', '19-30', '31-45', '46-60', '61-100', '100+']
            df['FAIXA_ETARIA'] = pd.cut(df['IDADE_PESSOA'], bins=bins, labels=labels).astype(str)
            demographic_features.extend(['IDADE_PESSOA', 'FAIXA_ETARIA'])

        categorical_features = [
            'SEXO_PESSOA', 'COR_PELE', 'TIPO_VEICULO', 'TIPO_LOCAL',
            'NATUREZA_APURADA', 'CIDADE', 'BAIRRO', 'UF'
        ]

        available_categorical = [col for col in categorical_features if col in df.columns]
        all_features = temporal_features + geographic_features + demographic_features + available_categorical
        available_features = [col for col in all_features if col in df.columns]

        print("üìã FEATURES SELECIONADAS:")
        feature_categories = {
            'Temporais': [f for f in temporal_features if f in available_features],
            'Geogr√°ficas': [f for f in geographic_features if f in available_features],
            'Demogr√°ficas': [f for f in demographic_features if f in available_features],
            'Categ√≥ricas': [f for f in available_categorical if f in available_features]
        }

        for category, features in feature_categories.items():
            if features:
                print(f"   ‚Ä¢ {category}: {len(features)} features")
                for feature in features:
                    unique_vals = df[feature].nunique()
                    print(f"     - {feature}: {unique_vals} valores √∫nicos")

        features_df = df[available_features].copy()
        self.feature_info = feature_categories

        print(f"\n‚úÖ Engenharia de features conclu√≠da: {features_df.shape}")
        return features_df

    def smart_encoding(self, features_df):
        """
        ‚úÖ CORRE√á√ÉO CR√çTICA 4: Codifica√ß√£o com normaliza√ß√£o robusta e PCA
        """
        print("\nüî† CODIFICA√á√ÉO INTELIGENTE DE FEATURES")
        print("=" * 45)

        X = features_df.copy()

        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = X.select_dtypes(include=['object']).columns.tolist()

        print(f"üî¢ Features num√©ricas ({len(numeric_features)}):")
        for feature in numeric_features:
            stats = X[feature].describe()
            print(f"   ‚Ä¢ {feature}: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.2f}")

        print(f"\nüè∑Ô∏è  Features categ√≥ricas ({len(categorical_features)}):")
        for feature in categorical_features:
            unique_count = X[feature].nunique()
            top_categories = X[feature].value_counts().head(3)
            print(f"   ‚Ä¢ {feature}: {unique_count} categorias")
            print(f"     Top: {', '.join([f'{k}({v})' for k, v in top_categories.items()])}")

        print("\nüîÑ Processando valores missing...")
        for col in categorical_features:
            X[col] = X[col].fillna('N√ÉO_INFORMADO')
            if X[col].nunique() > 20:
                top_categories = X[col].value_counts().head(15).index
                X[col] = X[col].apply(lambda x: x if x in top_categories else 'OUTROS')
                print(f"   ‚Ä¢ {col}: cardinalidade reduzida para 16 categorias")

        for col in numeric_features:
            X[col] = X[col].fillna(X[col].median())

        # ‚úÖ DETEC√á√ÉO DE OUTLIERS MULTIVARIADOS (antes da codifica√ß√£o)
        if len(numeric_features) > 0:
            inlier_mask = self.detect_multivariate_outliers(X[numeric_features].values)
            X = X[inlier_mask].copy()
            print(f"\n   ‚Ä¢ Dataset ap√≥s remo√ß√£o de outliers: {X.shape}")

        print("\nüéØ Aplicando codifica√ß√£o one-hot...")
        if categorical_features:
            X_encoded = self.encoder.fit_transform(X[categorical_features])
            encoded_features = self.encoder.get_feature_names_out(categorical_features)
            print(f"   ‚Ä¢ {len(categorical_features)} features ‚Üí {len(encoded_features)} colunas codificadas")
        else:
            X_encoded = sparse.csr_matrix((X.shape[0], 0))
            encoded_features = []

        # ‚úÖ NORMALIZA√á√ÉO ROBUSTA (resistente a outliers)
        print("\nüéØ Aplicando normaliza√ß√£o robusta (RobustScaler)...")
        if numeric_features:
            X_scaled = self.scaler.fit_transform(X[numeric_features])
            X_scaled = sparse.csr_matrix(X_scaled)
            print("   ‚Ä¢ RobustScaler aplicado (resistente a outliers remanescentes)")
        else:
            X_scaled = sparse.csr_matrix((X.shape[0], 0))

        X_final = sparse.hstack([X_scaled, X_encoded])

        print(f"\n‚úÖ Dataset antes do PCA: {X_final.shape}")
        print(f"   ‚Ä¢ Matriz esparsa: {X_final.getnnz():,} elementos n√£o-zero")
        print(f"   ‚Ä¢ Densidade: {X_final.getnnz() / (X_final.shape[0] * X_final.shape[1]):.4f}")

        # ‚úÖ REDU√á√ÉO DE DIMENSIONALIDADE COM PCA
        print("\nüéØ APLICANDO PCA PARA REDU√á√ÉO DE RU√çDO")
        print("=" * 45)
        
        X_dense = X_final.toarray()
        
        if X_dense.shape[1] > 10:  # S√≥ aplica PCA se tiver muitas features
            X_pca = self.pca.fit_transform(X_dense)
            explained_variance = self.pca.explained_variance_ratio_.sum()
            
            print(f"   ‚Ä¢ Dimens√µes originais: {X_dense.shape[1]}")
            print(f"   ‚Ä¢ Dimens√µes ap√≥s PCA: {X_pca.shape[1]}")
            print(f"   ‚Ä¢ Vari√¢ncia explicada: {explained_variance*100:.2f}%")
            print(f"   ‚Ä¢ Redu√ß√£o: {(1 - X_pca.shape[1]/X_dense.shape[1])*100:.1f}%")
            
            feature_names = [f'PC{i+1}' for i in range(X_pca.shape[1])]
            X_df = pd.DataFrame(X_pca, columns=feature_names, index=X.index)
        else:
            print("   ‚Ä¢ PCA n√£o aplicado (poucas features)")
            feature_names = list(numeric_features) + list(encoded_features)
            X_df = pd.DataFrame(X_dense, columns=feature_names, index=X.index)

        # Estat√≠sticas finais
        print(f"\nüìä RESUMO DE LIMPEZA DE OUTLIERS:")
        for key, value in self.outlier_stats.items():
            print(f"   ‚Ä¢ {key}: {value:,}")

        return X_df

    def save_preprocessing_artifacts(self):
        """Salva artefatos do pr√©-processamento"""
        joblib.dump(self.scaler, 'advanced_scaler.pkl')
        joblib.dump(self.encoder, 'advanced_encoder.pkl')
        joblib.dump(self.pca, 'advanced_pca.pkl')
        joblib.dump(self.feature_info, 'feature_info.pkl')
        joblib.dump(self.outlier_stats, 'outlier_stats.pkl')
        print("üíæ Artefatos de pr√©-processamento salvos")

    def full_pipeline(self, csv_path, sample_frac=None):
        """
        Pipeline completo com todas as corre√ß√µes aplicadas
        """
        # 1. Carregar dados
        df = self.load_and_analyze_data(csv_path, sample_frac)
        
        # 2. Limpeza cr√≠tica de coordenadas
        df = self.clean_geographic_outliers(df)
        
        # 3. Detec√ß√£o de outliers espaciais (LOF)
        df = self.detect_spatial_outliers(df)
        
        # 4. Criar visualiza√ß√µes
        self.create_eda_visualizations(df)
        
        # 5. Engenharia de features
        features_df = self.enhanced_feature_engineering(df)
        
        # 6. Codifica√ß√£o inteligente (inclui detec√ß√£o multivariada + PCA)
        X_final = self.smart_encoding(features_df)
        
        # 7. Salvar artefatos
        self.save_preprocessing_artifacts()
        
        return X_final