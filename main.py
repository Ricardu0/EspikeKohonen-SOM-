"""
Pipeline Principal - Arquivo de entrada
"""

import argparse
import logging
import joblib
import traceback
import numpy as np
import pandas as pd
from data.preprocessor import AdvancedDataPreprocessor
from models.som_trainer import MemoryEfficientSOMTrainer
from models.hyperparameter_optimizer import SOMHyperparameterOptimizer
from analysis.som_analyzer import KohonenAdvancedAnalyzer
from analysis.cluster_interpreter import SOMClusterInterpreter
from config.settings import RANDOM_STATE

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """Fun√ß√£o principal do pipeline avan√ßado"""
    try:
        parser = argparse.ArgumentParser(description='Pipeline Avan√ßado de Rede de Kohonen com An√°lise Interpret√°vel')
        parser.add_argument('--input', default='SPSafe_2022.csv', help='Arquivo CSV de entrada')
        parser.add_argument('--output', default='X_ready_advanced.parquet', help='Arquivo de sa√≠da')
        parser.add_argument('--sample_frac', type=float, default=0.3, help='Fra√ß√£o de amostragem (0.1-1.0)')
        parser.add_argument('--iterations', type=int, default=1000, help='Itera√ß√µes do SOM')  # ‚úÖ Reduzido
        parser.add_argument('--max_clusters', type=int, default=12, help='N√∫mero m√°ximo de clusters')
        parser.add_argument('--map_size', type=int, default=20, help='Tamanho do mapa (opcional)')  # ‚úÖ Reduzido
        parser.add_argument('--sigma', type=float, default=1.0, help='Sigma do SOM')
        parser.add_argument('--learning_rate', type=float, default=0.5, help='Taxa de aprendizado')
        parser.add_argument('--optimize', action='store_true', help='Otimizar hiperpar√¢metros automaticamente')
        parser.add_argument('--fast_optimize', action='store_true', help='Otimiza√ß√£o r√°pida (menos combina√ß√µes)')  # ‚úÖ Nova op√ß√£o

        args = parser.parse_args()

        logger.info("=" * 70)
        logger.info("üß† PIPELINE AVAN√áADO - REDE DE KOHONEN (SOM PURO)")
        logger.info("=" * 70)

        # 1. PR√â-PROCESSAMENTO AVAN√áADO
        logger.info("üéØ FASE 1: PR√â-PROCESSAMENTO E AN√ÅLISE EXPLORAT√ìRIA")
        preprocessor = AdvancedDataPreprocessor()

        # Primeiro carregamos o dataframe original para ter as colunas originais
        try:
            df = pd.read_csv(args.input, sep=';', encoding='utf-8', low_memory=False)
        except UnicodeDecodeError:
            df = pd.read_csv(args.input, sep=';', encoding='latin-1', low_memory=False)

        # Aplicar amostragem se necess√°rio
        if args.sample_frac and args.sample_frac < 1.0:
            df = df.sample(frac=args.sample_frac, random_state=42)
            logger.info(f"üìä Aplicada amostragem: {args.sample_frac*100}% dos dados")

        # CORRE√á√ÉO: Agora passamos o caminho do arquivo para o preprocessor
        X_processed = preprocessor.full_pipeline(args.input, args.sample_frac)

        # IMPORTANTE: O full_pipeline remove outliers, ent√£o temos menos linhas
        # Precisamos sincronizar df com X_processed
        # Se X_processed tiver √≠ndice, usamos ele (assumindo que a ordem foi mantida)
        if hasattr(X_processed, 'index'):
            try:
                # Tentamos alinhar pelo √≠ndice
                df = df.iloc[X_processed.index].copy()
                logger.info("‚úÖ Dados originais alinhados com dados processados pelo √≠ndice")
            except:
                # Fallback: usar o mesmo n√∫mero de linhas
                df = df.head(len(X_processed)).copy()
                logger.warning("‚ö†Ô∏è  Usando fallback para alinhamento de dados")
        else:
            # X_processed n√£o tem √≠ndice (array numpy)
            df = df.head(len(X_processed)).copy()
            logger.warning("‚ö†Ô∏è  X_processed n√£o tem √≠ndice, usando fallback")

        X_processed.to_parquet(args.output, index=False)
        logger.info(f"üíæ Dados processados salvos: {args.output}")
       
        # 2. TREINAMENTO DA REDE DE KOHONEN
        logger.info("üéØ FASE 2: TREINAMENTO DA REDE DE KOHONEN")

        data_for_training = X_processed.values.astype(np.float32)

        if args.optimize or args.fast_optimize:
            logger.info("   üîß Executando otimiza√ß√£o de hiperpar√¢metros...")
            optimizer = SOMHyperparameterOptimizer(random_state=RANDOM_STATE)

            # ‚úÖ Grade de par√¢metros adapt√°vel
            if args.fast_optimize:
                param_grid = {
                    'som_x': [15, 20],  # ‚úÖ Menos op√ß√µes
                    'som_y': [15, 20],
                    'sigma': [0.8, 1.0],
                    'learning_rate': [0.3, 0.5],
                    'iterations': [500, 1000]  # ‚úÖ Menos itera√ß√µes
                }
                max_evaluations = 8  # ‚úÖ Menos avalia√ß√µes
            else:
                param_grid = {
                    'som_x': [20, 25, 30],
                    'som_y': [20, 25, 30],
                    'sigma': [0.8, 1.0, 1.2],
                    'learning_rate': [0.3, 0.5, 0.7],
                    'iterations': [1000, 2000, 3000]
                }
                max_evaluations = 15

            best_params = optimizer.optimize_parameters(
                data_for_training, param_grid, max_evaluations
            )

            # Usar melhores par√¢metros
            kohonen_trainer = MemoryEfficientSOMTrainer(random_state=RANDOM_STATE)
            som, q_error, t_error = kohonen_trainer.train_kohonen_network(
                data_for_training, **best_params
            )
        else:
            # Usar par√¢metros manuais
            kohonen_trainer = MemoryEfficientSOMTrainer(random_state=RANDOM_STATE)
            som, q_error, t_error = kohonen_trainer.train_kohonen_network(
                data_for_training,
                som_x=args.map_size,
                som_y=args.map_size,
                sigma=args.sigma,
                learning_rate=args.learning_rate,
                iterations=args.iterations
            )

        logger.info(f"‚úÖ Treinamento conclu√≠do: QE={q_error:.4f}, TE={t_error:.4f}")

        # 3. VISUALIZA√á√ïES AVAN√áADAS
        logger.info("üéØ FASE 3: VISUALIZA√á√ïES E AN√ÅLISES")
        analyzer = KohonenAdvancedAnalyzer()
        analyzer.create_comprehensive_visualizations(som, X_processed)

        # 4. AN√ÅLISE DE CLUSTERS
        logger.info("üéØ FASE 4: AN√ÅLISE DE CLUSTERS (SOM PURO)")
        interpreter = SOMClusterInterpreter(preprocessor, kohonen_trainer, analyzer)
        df_with_clusters, quality_metrics = interpreter.analyze_som_clusters(
            X_processed, df, args.max_clusters
        )

        # Salvar resultados finais
        df_with_clusters.to_parquet('dataset_com_clusters_som.parquet', index=False)
        joblib.dump(kohonen_trainer.som, 'kohonen_model_pure_som.pkl')

        if quality_metrics:
            joblib.dump(quality_metrics, 'cluster_quality_metrics.pkl')

        logger.info("üéâ PIPELINE CONCLU√çDO COM SUCESSO!")

    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è  Pipeline interrompido pelo usu√°rio")
    except Exception as e:
        logger.error(f"‚ùå Erro no pipeline: {e}")
        logger.error(traceback.format_exc())
        raise

    if 'LATITUDE' in df_with_clusters.columns:
        lat_mean = df_with_clusters['LATITUDE'].mean()
        lon_mean = df_with_clusters['LONGITUDE'].mean()

        if abs(lat_mean) > 100 or abs(lon_mean) > 100:
            logger.error("‚ùå COORDENADAS AINDA CORROMPIDAS!")
            logger.error(f"   Latitude m√©dia: {lat_mean:.0f}")
            logger.error(f"   Longitude m√©dia: {lon_mean:.0f}")
            logger.error("   A√á√ÉO: Verificar preprocessor linha ~80")
        else:
            logger.info(f"‚úÖ Coordenadas OK: Lat={lat_mean:.2f}, Lon={lon_mean:.2f}")

    # Verificar clusters
    n_clusters = df_with_clusters['CLUSTER_SOM'].nunique() - 1  # -1 para excluir ru√≠do
    if n_clusters < 3:
        logger.warning(f"‚ö†Ô∏è  Poucos clusters: {n_clusters}")
        logger.warning("   SUGEST√ïES:")
        logger.warning("   1. Aumentar map_size (atual: 30 ‚Üí testar 40)")
        logger.warning("   2. Reduzir sigma (atual: 1.5 ‚Üí testar 1.0)")
        logger.warning("   3. Aumentar iterations (atual: 5000 ‚Üí testar 8000)")
    else:
        logger.info(f"‚úÖ Clusters identificados: {n_clusters}")

if __name__ == '__main__':
    main()

    # Instru√ß√£o para rodar o script:
    # python main.py --input SPSafe_2022.csv --output X_ready_advanced.parquet --sample_frac 0.3 --iterations 1000 --max_clusters 12 --map_size 20 --sigma 1.0 --learning_rate 0.5 --optimize
    # python main.py --input SPSafe_2022.csv --sample_frac 0.1 --iterations 300 --map_size 10
    # python main.py --input SPSafe_2022.csv --sample_frac 0.3 --optimize